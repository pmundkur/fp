  - Arrays are different from vectors:

    . elements of a vector (values of primitive types) are of a
      well-defined constant size, and since modifying them does not
      change the size of any containing structure, they can be
      modified in place.
    . elements of an array are structures, and do not have a
      well-defined size; since modifying them can result in a change
      in their size, and that of their containing structure, they
      cannot be modified in place.

    This means that arrays and vectors need different functors.

    Also, for this reason, the parser return values for arrays and
    vectors and primitive types are different.  For vectors and
    primitive types, a parser returns a structure of typed
    pointers (M.t) to the input data:
    . the pointers need to be read (M.read) to get the values, but
      the input buffer is largely untouched until the pointers are read
    . the underlying buffer can be modified in place
      (read-modify-write does not involve an extra copy)
    . but the gc-lifetime of the buffer is tied to the lifetime of
      the returned structure value

    For arrays, a parsers returns a structure of values, and there are
    no pointers to structures, so entire structures cannot be modified
    in place (their individual scalar fields and vector elements can,
    as described above.)

- A note on arguments to offset().

   . for dependency analysis/generation facet:

     Struct construction always requires that all vector/array fields
     be specified, since they cannot be given value attributes.  This
     means that all fields of variable length will have their lengths
     known at construction time.  This allows offsets of all fields in
     the struct to be known at construction time.  Hence, the use of a
     field as an offset argument cannot create a dependency for it.
     This is why when we create dependencies from a length expression
     below, we omit offset arguments.

   . for the parser generation/parsing facet:

     Since the offset of a field in a struct being parsed cannot be
     computed knowing only its name (and any API in the runtime
     library), we need to generate a variable to record the offset of
     any field used as an offset() argument at the time it is parsed.

     This variable will be used to pass the offset value to any users
     of the offset, both by sibling fields, as well as generated
     parser functions for embedded structs which use this offset.

     This means we need to record the following:

     . for each struct which of its fields occur as an argument to
       offset() (including occurences in embedded structs).

     . for each struct, which of its dependency variables occur in the
       context of an argument to offset().  This is so that the
       generated parser function can receive a corresponding variable
       containing the offset-value.

- Notes on dependency-variables for parsing codegen:

  Essentially, information of field identifier occurences is needed
  along two axes:

  . what context they appear in: either a context in which their type
    is all that matters (normal contexts), or as in argument to
    offset(), where their type does not matter, and hence they need to
    be handled specially during parsing.  Note that identifiers could
    appear in both contexts.

  . whether they appear in a generator expression for a field (e.g. a
    vector or array length, or the branch field of a classify map), or
    whether they are free variables of an embedded struct.  Note that
    a field could have both generator fields as well as fields free in
    its associated struct.

- Notes on dependencies

  It's not enough to know what the dependency variables are, but we
  need to know how they occur in the expression(s), and how they are
  related to the field associated with that expression.

  Dependency variables of a struct that are needed for the parse phase
  appear in:

  . vector and array length expressions
  . classification expressions

  Additional variables for a {array,map}-valued field occur in
  the field's array length or classify expression.  These
  variables are not needed as arguments to the generated struct
  parsing functions, but are needed to compute the length or branch of
  the field itself.

    For example, in

      format f {
        br  : byte;
        len : byte;
        s : ...  {
           g : array (br) {
                k : byte[len];
               }
        }
      }

    len is free in the struct G, and hence is an argument to its
    generated parsing function.  br is needed in the generated code
    that sets up field g itself.  These two groups of variables
    need to be stored seperately for field g.

    Both br and len are free in the struct S of field s, and hence
    belong to the same group in this case.

  remaining() is a special free-variable, whose name remains the same
  but whose value changes for each struct.  Its value is not derived
  from any field, but from the env/buffer passed in to the generated
  parsing function.

  Within these expressions, a variable can appear in one or more
  of the following contexts:

  1. as a direct argument to one of the arithmetic functions.

    Currently all these functions take integer arguments.  In this
    case, the generated struct parsing function needs only the type of
    the variable.  The integer argument for the arithmetic
    function can be computed by using the _to_int API of the module
    corresponding to the field type from the runtime library.

    The value used for the call to this generated parsing function
    will be the name of the representation of the corresponding field.

    A special case of this is when a variable directly appears as the
    length expression of a vector or array, e.g. the variable
    "len" in
         vec: byte[len]
         arr: array(len) {...}

  2. as a direct argument to the "length" or "array_length" function.

    The generated struct parsing function only needs the type of the
    variable.  The length value can then be computed by a call to
    the appropriate vector/array length function from the runtime
    library.

    The value used for the call to this generated parsing function
    will be the name of the representation of the corresponding field.

  3. as a direct argument to the "offset" function.

    This case differs from the above two, since the generated struct
    parsing function will need the value of the offset, since it
    cannot determine the offset value from either or both of the
    variable name/type using (current incarnation of) the runtime
    library.

    The variable used for the call to this parsing function will have
    to be generated and its name recorded in the codegen state.  (We
    could always generate and record this for each field, irrespective
    of whether it was used as an offset, but this would not be very
    elegant.)

    To compute the offset, we would need to record the variable
    holding the env of nearest enclosing struct in the codegen
    environment.  It is simpler to do this irrespective of whether or
    not the variable is needed later for offset computation.

  Note that variables falling under categories 1) and 2) are mutually
  exclusive, whereas variables falling under 3) could also fall under
  1) or 2).  Hence, the generated parsing functions for a struct could
  take multiple arguments derived from the same free field, and
  generated calls to these functions could need multiple variables to
  hold the arguments for these functions.

  remaining(): The use of this "free-variable" does not need a
  corresponding variable to be recorded in the codegen environment.
  The codegen merely generates a call to Env.remaining with the
  current env as argument.

- Notes on parsing and generation phases

  Taking a step back, it is useful to keep in mind that each format
  specification has two facets: a parsing facet, and a generation
  facet.

  There are various stages to checking a format specification:

  . type checking the parsing facet

  . type checking the generation facet

    We do both these stages together.

  . sanity checking the generation facet

    This ensures that formats satisfying the value attributes of the
    fields in the specification can actually be generated, given
    appropriate inputs.  This primarily involves checking for circular
    dependencies in the value expressions, or redundant value
    expressions.

    Redundant value expressions result when the user provides value
    expressions for fields (say, f) that are the focus of classify
    expressions (say, for field g).  That is, we have,

      f : byte;
      g : classify (f) {
          |1:case1 -> ...
          |2..5:case2 -> ...
          ...
          }
      }

    Such fields (g) have an implicit value expression that computes
    the value based on the case value of f.  In the example above, f
    has an implicit branch case value (|g=case1 -> 1).  This could
    conflict with what the user provides, so we need to check such
    value expressions, which is simple to check for.  In case the
    branch is a range (e.g. case2 above), then the field must specify
    a value attribute (since we cannot compute a unique value for f in
    this case, the user needs to specify one).  If a value attribute
    is specified, it should be consistent (i.e. the values should
    match in the const cases, and be within range in the non-const
    cases.)

    We now discuss circular dependencies.

    The scheme to do this uses a fairly obvious dependency graph
    approach, but is noted here for completeness.  We treat each value
    expression as a single-level directed tree (i.e. consisting of
    only a root node and leaf nodes, with edges being directed from
    the root to the leaves).  The leaf nodes in this tree are of two
    types: the first type are the variables in the expression that are
    references to the other fields of the format.  (A field cannot
    refer to itself in its value expression for obvious reasons.)  We
    also introduce shadow variables for each array or vector valued
    field which denote the length of that array or vector.  Value
    expressions that have a sizeof(field) subexpression will cause the
    shadow variable for field to be added as a leaf node to the tree.

    Note that:

    . shadow variables do not appear in the specification, and hence
      do not have value expressions, and cannot be the root nodes of
      any expression trees.

    . fields with array or vector types cannot have value expressions
      (this is ensured by the type checker, and so there's no need to
      enforce this at a syntactic level.

    These expression trees are then stitched together by attaching the
    root node of a field's expression tree to other trees where that
    field appears as a leaf node.  If the resulting graph has any
    cycles, then we immediately have an unconstructible format.  (A
    field refering to itself in its value expression is a special case
    of this.)

  . checking that the parsing facet and the generation facet describe
    the same format

    This step is limited in what it can do.  To guarantee that the two
    facets match, we need to either rely on programmers not making
    mistakes, or severely limit the expressible formats.  The latter
    is not advisable, since it can actually be useful to allow the
    generation of formats that do not follow the parsing
    specification.  Generating invalid formats can be used to test the
    error handling of the parsing code.

    So this checking stage is used to autocompute as many fields of
    the format for the generation facet in a manner consistent with
    the parsing facet of the specification; if we cannot do this, we
    merely warn about possible inconsistencies, and fall back to
    requiring the programmer to explicitly provide the field values.

    Fields for which auto-computation is possible are:

    - fields used for classification branches, provided the branch
      expressions are constants

    - fields that occur in vector length expressions, provided the
      length expression is a single variable

    - fields used in array length expressions, provided the array
      length expression is a single variable

    For example, we would like to check is that in cases of

    format vec { len : int; arr = byte[len] }

    the value for the len field is autocomputed by the generation
    function for vec from its argument value for the arr value.  This
    would statically ensure that we don't generate a format where the
    len value does not match the length of the arr vector.  Needless
    to say, we cannot autocompute array or vector valued fields.

    To implement the checking described in this section, the
    dependency graph described above is extended by ensuring that the
    shadow variable of a array/vector typed field (e.g. arr) is a
    direct child of fields (e.g. len) that appear in its array length
    expression; this dependency is marked using a special edge.  This
    ensures that the programmer has specified a value expression
    taking into account the parsing-generation linkage, even though we
    cannot verify its correctness.  If there is no such direct child
    relationship, then we emit a warning, and do not emit autocompute
    code for the field (i.e. len).

    Here's an example of a format that would pass this check, but
    violates parsing-generation correctness:

    format { len: int value(sizeof(arr1) + sizeof(arr2));
             arr1: int[len];
             arr2: byte[len+2]; }

    The programmer can generate messages that would cause the
    corresponding parser to parse garbage.

  A note on scoping for nested formats: This differs for the parsing
  and the generation facets.  The reason for this is that during
  parsing, the formats are constructed from outside in.  Hence, any
  expressions used during parsing (e.g vector and array length
  expressions) can contain free variables referring to the embedding
  format.  However, the generation facet constructs formats from
  inside out: a nested format does not know where it will be embedded,
  and hence any expressions used during generation (e.g. value
  expressions) cannot (i.e. _should_ not: this needs to be enforced
  during typechecking) contain free variables referring to embedding
  formats.  In other words, this is an implementation of the layering
  principle, and having enclosed formats referring to enclosing
  formats is a layering violation.

- Notes on implementation:

  . A bytestream view is implemented as an abstract datatype in its own
    module.

  . Each datatype is considered to be an interpretation of a view of a
    bytestream.

    To implement this, the representation types for the datatype modules
    are the view, not the evaluation type.  The evaluation type needs to
    also be exported in the module signature, to implement vector
    functors (see below).

  . The vector datatype is implemented as a functor, that takes as an
    argument the module for a primitive datatype.

  . The representation type of a dependent sum will be an object of a
    class whose constructor will take as arguments the representation
    types of each label.

  . Lib: We handle errors that arise when unmarshalling beyond the limit
    of the buffer by throwing an appropriate exception.  This way, the
    user can retry with a bigger buffer.

  . We use a Map kind for classification fields.

- Output of the typechecking stage:

  This is essentially a cooked version of the ast:

  . Variable names are resolved into identifiers, so that lexical
    scoping is resolved.  Note that variables in expressions are
    always references, not definitions.

  . Expressions are constant folded.

  . Expressions in variant definitions are entered into the
    environment in their AST form, since they cannot be checked or
    constant folded without knowing their required type.  The type is
    only known when they are associated with a field; so the variant
    definition is extracted from the environment when processing such
    a field, and is then type checked and constant-folded in the
    context of the field that uses it.

    Expressions in every other context (within a format definition)
    have a type deduced for it; these contexts are (i) as arguments to
    a function, (ii) as values in value attributes, and (iii) as
    values of inline variant definitions.

- Notes on value() attributes:

  value() attributes can handle assignments for multiple cases
  for possibly multiple branches, with support for both nested and
  serial branching.

  Example A:

  len   : int  value (|f=n1,f.g=m1 -> sizeof(f.g.fi1)
                      |f=n1,f.g=m2 -> sizeof(f.g.fi2)-1
                      |f=n2        -> e
                      | _          -> e')

  case1 : int  value (|f=n1,f.g=m1 -> (eo1)
                      |f=n1,f.g=m2 -> (eo1)
                      |f=n2        -> (eo2))
  case2 : int  value (|f=n1,f.g=m1 -> ei1
                      |f=n1,f.g=m2 -> ei2
                      | _          -> e')    (* Needed for f != n1)
  f     : classify (case1) {
          |eo1:n1 -> {
                        g : classify (case2) {
                            |ei1:m1 -> fi1 : int[len]
                            |ei2:m2 -> fi2 : int[len+1]
                        }
                     }
          |eo2:n2 -> {f2}
  }

  Example B:

  case1 : int  value(|f=n1 -> ef1 |f=n2 -> ef2)
  case2 : int  value(|g=m1 -> es1 |g=m2 -> es2)
  len1  : int  value(|f=n1 -> sizeof(f.g) |g=m2 -> sizeof(g.l))
  len2  : int  value(|f=n2 -> sizeof(f.h) |g=m1 -> sizeof(g.k))

  f  : classify (case1) {
       |ef1:n1 -> g : int[len1];
       |ef2:n2 -> h : int[len2];
  }

  g  : classify (case2) {
       |es1:m1 -> k : int[len2];
       |es2:m2 -> l : int[len1];
  }


  . When checking the attributes, we ensure that the cases specified
    for a value attribute are contained in the branching tree.  If no
    cases are specified, it is equivalent to the use of a default
    ("_") case.

  . We also check to ensure that the case specification is complete
    (i.e. all branches are covered), and do not contain unmatched
    cases.

  . TODO: check for conflict: e.g. len1 when f=n1 and f=m2

- Notes on overlap checking:

  We check for overlaps in classification branches.  This is a
  generation-only check.

  The algorithm for this is essentially derived from the one specified
  in Luc Maranget's "Warnings" paper.  The way we adapt it for our
  purposes is to consider each classified struct as a variant datatype
  with a tuple argument.  The root struct can be considered as an
  anonymous tuple.  An example explains:

  struct {
     a : int
     b : int value ()
     c : classify (b) {
           0:A -> {
             f : int
             g : classify (f) {
                    0:F -> { }
                    1:G -> { }
                 }
             h : byte
           }
           1:B -> {
             m : int
             n : classify (m) {
                    0:M -> { }
                    1:M -> { }
                 }
             o : int
           }
         }
     d : int
     k : classify (d) {
           0: D -> { }
           1: E -> { }
         }
  }

  This is equivalent to the following variant datatypes for pattern
  matching purposes:

  root struct = (c * k)
  and c = A of g | B of n
  and k = D | E
  and g = F | G
  and n = M | N

- Sanity checks for generation:

  We add a generation-only check to ensure that a field is not the
  target of classification multiple times, since this complicates
  generation checking:

  g : classify (f) {
       0:A -> { }
      }
  h : classify (f) {
       1:B -> { }
      }

  Then

     generate ('A g) ('B h) ...

  would result in conflicting values for f: 0 and 1.  Otherwise we
  would need to catch this case via code generation in the body for
  'generate'.  This in conjunction with ranges for classification
  branches would complicate generation checking.

  This check can be removed later via a command-line option: this
  would result in generation code that remove such branching fields
  from the auto-computed set.
